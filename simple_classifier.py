import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from spektral.layers import GraphConv, GlobalMaxPool
from spektral.utils import conversion
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

import sys
from utils.category_filter import parse_category
from utils.networkx_converter import get_nx_graphs
from utils.node_labels import get_labels
import random


def __get_graph_list(files: list):
    """
    Given a list of files, returns a list of networkx graphs representing a person pose
    :param list files: list of JSON files
    :return list: list of networkx graphs
    """
    graph_list = []
    for file in files:
        graph_list.extend(get_nx_graphs("../data/" + file, ['pose'], normalize=True))
    return graph_list


def __standardize_data(adj_matrix, var_matrix, out_list):
    """
    Given a list of adj matrices, a list of matrices representing the predictor values for each graph, and a list of
    outputs, remove the graphs with mean precision is greater than 50%, subtracts the mean and divide with the std
    :param numpy array adj_matrix: numpy array of adj matrices
    :param numpy array var_matrix: numpy array of matrix-predictors
    :param numpy array out_list: list of outputs
    :return 3x numpy arrays: A_norm, X_norm and y_norm normalized
    """
    fil = np.full(out_list.shape[0], fill_value=True)
    for i in range(out_list.shape[0]):
        fil[i] = (var_matrix[i, :, 2] > .7).mean() > .5
    adj_matrix_std, var_matrix_std, y_std = adj_matrix[fil], var_matrix[fil], out_list[fil]
    # labels_matrix = generate_labels(var_matrix_std.shape[0])
    var_matrix_std = var_matrix_std[:, :, :2]
    var_matrix_std -= var_matrix_std.mean(axis=1, keepdims=True)
    var_matrix_std /= var_matrix_std.std(axis=1, keepdims=True)
    return adj_matrix_std, var_matrix_std, y_std


def generate_labels(n_graphs):
    label_list = get_labels('pose')
    label_array = np.array([x['label'] for x in label_list])
    return np.tile(label_array, n_graphs).reshape(n_graphs, 25)


def generate_data():
    """
    Generates matrices representations of networkx graphs, matrices of predictors and matrix of outputs.
    :return: list of matrices(graphs), list of matrices of predictors, list of outputs all normalized
    """
    print('Retrieving categories...', end='')
    bicycling_files = parse_category("bicycling")
    running_files = parse_category("running")
    print('done')

    print('Generating networkx graphs...', end='')
    bicycling_graphs = __get_graph_list(bicycling_files)
    running_graphs = __get_graph_list(running_files)
    print('done')

    print('Converting to numpy matrix...', end='')
    bicycling_matrix = conversion.nx_to_numpy(bicycling_graphs, nf_keys=['x', 'y', 'confidence'])
    running_matrix = conversion.nx_to_numpy(running_graphs, nf_keys=['x', 'y', 'confidence'])
    A = np.append(bicycling_matrix[0], running_matrix[0], axis=0)
    X = np.append(bicycling_matrix[1], running_matrix[1], axis=0)
    y = np.append(np.repeat([[1.0, 0.0]], bicycling_matrix[0].shape[0], axis=0),
                  np.repeat([[0.0, 1.0]], running_matrix[0].shape[0], axis=0), axis=0)
    print('done')
    return __standardize_data(A, X, y)


def plot_results(fit_res):
    """
    Plots accuracy and loss for both train and test datasets.
    :param fit_res: dictionary generated by keras.fit
    """
    # Plot training & validation accuracy values
    plt.subplot(2, 1, 1)
    plt.plot(fit_res.history['acc'])
    plt.plot(fit_res.history['val_acc'])
    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')

    # Plot training & validation loss values
    plt.subplot(2, 1, 2)
    plt.plot(fit_res.history['loss'])
    plt.plot(fit_res.history['val_loss'])
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.subplots_adjust(hspace=0.5)
    plt.show()


if __name__ == "__main__":
    generate_labels(12)
    A, X, y = generate_data()

    # Parameters
    n_nodes = X.shape[-2]
    n_variables = X.shape[-1]
    n_classes = y.shape[-1]
    learning_rate = 1e-4
    epochs = 20000
    batch_size = 64
    es_patience = 200
    lr_patience = 5

    # Create train and test datasets 9863
    A_train, A_test, x_train, x_test, y_train, y_test = train_test_split(A, X, y, test_size=0.2)

    # Create model
    X_in = Input(shape=(n_nodes, n_variables))
    A_in = Input((n_nodes, n_nodes))
    gc = GraphConv(64, activation='relu')([X_in, A_in])
    gc = GraphConv(64, activation='relu')([gc, A_in])
    gc = GraphConv(64, activation='relu')([gc, A_in])
    pool = GlobalMaxPool()(gc)
    output = Dense(n_classes, activation='softmax')(pool)
    model = Model(inputs=[X_in, A_in], outputs=output)
    optimizer = Adam(lr=learning_rate)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])
    # model.summary()

    # Train model
    # reduce_lr = ReduceLROnPlateau(monitor='val_acc', mode='max', factor=0.1, patience=lr_patience, min_lr=1e-6)
    early_stopping = EarlyStopping(patience=es_patience, restore_best_weights=True)
    print('Training the model...', end='')
    history = model.fit([x_train, A_train], y_train, batch_size=batch_size, validation_split=0.1, epochs=epochs,
                        callbacks=[early_stopping], verbose=0)
    print('done')

    #  Evaluate model
    print('Evaluating model...', end='')
    eval_results = model.evaluate([x_test, A_test], y_test, batch_size=batch_size, verbose=0)
    print('done')
    print('Test loss: {:.4f}. Test acc: {:.1f}%'.format(eval_results[0], eval_results[1] * 100))

    # plot_results(history)
